{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer：Transformer 架构是一种完全基于自注意力机制的神经网络架构，完全不需要递归和卷积。它由编码器和解码器组成，每个编码器由多层自注意力机制和前馈神经网络组成。\n",
    "\n",
    "### BERT：BERT 是专为自然语言理解任务设计的模型。它基于 Transformer 架构，特别是编码器部分。BERT 通过以自我监督的方式训练大量未标记的文本数据来学习上下文单词表示。它本质上是双向的，这意味着它在编码单词时可以考虑左右上下文。\n",
    "\n",
    "### GPT：另一方面，GPT 是为自然语言生成任务而设计的。它也基于 Transformer 架构，但仅使用解码器部分。GPT 通过在给定前面上下文的情况下预测序列中的下一个单词来学习生成文本。它是单向的，这意味着它只能从左到右生成文本。\n",
    "\n",
    "### 综上所述，BERT和GPT都是Transformer架构的具体实例，其中BERT专注于语言理解任务，GPT专注于语言生成任务。它们的不同之处在于如何利用 Transformer 组件和培训目标。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最新的 GPT-3 等模型接受了 45TB 数据的训练，包含 1750 亿个参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
